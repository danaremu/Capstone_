{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192b5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51bc19",
   "metadata": {},
   "source": [
    "# DATA INGESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9ab01",
   "metadata": {},
   "source": [
    "LOAD DATA FROM DATA FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "055acaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "data = {}\n",
    "data_names = []\n",
    "non_file = []\n",
    "base_folder = \"./../pseudo_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa112c",
   "metadata": {},
   "source": [
    "DECLARE ALL INGESTION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71016d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isFile(path):\n",
    "    global non_file\n",
    "    if os.path.isfile(path):\n",
    "        return True\n",
    "    else:\n",
    "        non_file.append(path)\n",
    "        return False\n",
    "\n",
    "\n",
    "def getAllFilesInDirectory(directory):\n",
    "    \"\"\"\n",
    "    Get all files in a directory.\n",
    "    \"\"\"\n",
    "    global non_file\n",
    "    data = [os.path.join(directory + \"/\", f) for f in os.listdir(directory) if isFile(os.path.join(directory + \"/\", f))]\n",
    "\n",
    "    while len(non_file) > 0:\n",
    "        data.extend(getAllFilesInDirectory(non_file.pop(0)))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Get data from files\n",
    "def getDataFromFile(files):\n",
    "    \"\"\"\n",
    "    Get data from a file.\n",
    "    \"\"\"\n",
    "    # global data_names, data\n",
    "    data_names_, data_ = [], {}\n",
    "\n",
    "    def getDataFromFile_(file):\n",
    "        if file.endswith(\".csv\"):\n",
    "            return pd.read_csv(file)\n",
    "        elif file.endswith(\".json\"):\n",
    "            return pd.read_json(file)\n",
    "        elif file.endswith(\".xlsx\"):\n",
    "            return pd.read_excel(file)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "    for file in files:\n",
    "        nom = file[file.rfind(\"/\")+1:-4]\n",
    "        data_names_.append(nom)\n",
    "        data_[nom] = getDataFromFile_(file)\n",
    "        print(f\"⬇️ Loading `{nom}`   ---   {file}\")\n",
    "        \n",
    "    print(\"\\n\", \"✅ All data loaded successfully\")\n",
    "    return (data_names_, data_)\n",
    "    \n",
    "    # return data[data_names[0]].info(), data[data_names[0]].head(), data[data_names[0]].describe(), data[data_names[0]].isnull().sum(), data[data_names[0]].dtypes, data[data_names[0]].columns, data[data_names[0]].shape, data[data_names[0]].nunique(), data[data_names[0]].duplicated().sum(), data[data_names[0]].sample(5), data[data_names[0]].sample(5).to_dict(), data[data_names[0]].sample(5).to_json()\n",
    "    # return data[data_names[0]].info(), data[data_names[0]].describe(), data[data_names[0]].isnull().sum(), data[data_names[0]].dtypes, data[data_names[0]].columns, data[data_names[0]].shape, data[data_names[0]].nunique(), data[data_names[0]].duplicated().sum(), data[data_names[0]].sample(5), data[data_names[0]].sample(5).to_dict(), data[data_names[0]].sample(5).to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6934db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading admins   ---   ./../pseudo_data/admins.csv\n",
      "Loading analytics_cache   ---   ./../pseudo_data/analytics_cache.csv\n",
      "Loading audit_logs   ---   ./../pseudo_data/audit_logs.csv\n",
      "Loading companies   ---   ./../pseudo_data/companies.csv\n",
      "Loading inventory   ---   ./../pseudo_data/inventory.csv\n",
      "Loading items   ---   ./../pseudo_data/items.csv\n",
      "Loading permissions   ---   ./../pseudo_data/permissions.csv\n",
      "Loading sales   ---   ./../pseudo_data/sales.csv\n",
      "Loading sale_items   ---   ./../pseudo_data/sale_items.csv\n",
      "Loading stock_entries   ---   ./../pseudo_data/stock_entries.csv\n",
      "Loading system_roles   ---   ./../pseudo_data/system_roles.csv\n",
      "Loading system_users   ---   ./../pseudo_data/system_users.csv\n",
      "Loading transaction_logs   ---   ./../pseudo_data/transaction_logs.csv\n",
      "Loading users   ---   ./../pseudo_data/users.csv\n",
      "Loading user_roles   ---   ./../pseudo_data/user_roles.csv\n",
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Get all files in the directory\n",
    "files = getAllFilesInDirectory(base_folder)\n",
    "# print(files)\n",
    "(data_names, data) = getDataFromFile(files)\n",
    "# print(data_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c42863",
   "metadata": {},
   "source": [
    "FORMAT DATA (REMOVE id FROM ALL DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e163a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAllDatasetIds(dataset, names):\n",
    "    for name in names:\n",
    "        dataset[name] = dataset[name].iloc[:, 1:]\n",
    "\n",
    "    print(\"✅ Data Transformed Successfully!\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e144d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = removeAllDatasetIds(data, data_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31dd382",
   "metadata": {},
   "source": [
    "LOAD DATA TO DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd26353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import logging\n",
    "import sqlalchemy\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = logging.basicConfig(filename=\"../capstone.log\", level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = None # To initialise the engine globally\n",
    "\n",
    "def databaseConnection():\n",
    "    global engine\n",
    "    if engine is None:\n",
    "        try:\n",
    "            with open('conn.yaml', mode='r') as file:\n",
    "                config = yaml.safe_load(file)\n",
    "\n",
    "                db_url = sqlalchemy.URL.create(\n",
    "                    drivername='postgresql+psycopg2', \n",
    "                    username=config['user'], \n",
    "                    password=config['password'], \n",
    "                    host=config['host'], \n",
    "                    port=config['port'], \n",
    "                    database=config['database']\n",
    "                )\n",
    "                engine = sqlalchemy.create_engine(db_url)\n",
    "                print(\"✅ Connected Successfully!\", \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to connect to database: {e}\")\n",
    "            raise\n",
    "        \n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploadToDatabase(dataset, names, engine):\n",
    "    for name in names:\n",
    "        try:\n",
    "            dataset[name].to_sql(name=name, con=engine, if_exists='append', index=False)\n",
    "            print(f\"✅ Uploaded `{name}` dataset successfully.\")\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"❌ Failed to upload `{name}` dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e08cb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected Successfully!\n",
      "✅ Uploaded `admins` successfully.\n",
      "✅ Uploaded `analytics_cache` successfully.\n",
      "✅ Uploaded `audit_logs` successfully.\n",
      "✅ Uploaded `companies` successfully.\n",
      "✅ Uploaded `inventory` successfully.\n",
      "✅ Uploaded `items` successfully.\n",
      "✅ Uploaded `permissions` successfully.\n",
      "✅ Uploaded `sales` successfully.\n",
      "✅ Uploaded `sale_items` successfully.\n",
      "✅ Uploaded `stock_entries` successfully.\n",
      "✅ Uploaded `system_roles` successfully.\n",
      "❌ Failed to upload `system_users`\n",
      "✅ Uploaded `transaction_logs` successfully.\n",
      "✅ Uploaded `users` successfully.\n",
      "✅ Uploaded `user_roles` successfully.\n"
     ]
    }
   ],
   "source": [
    "databaseConnection()\n",
    "uploadToDatabase(data, data_names, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb26b0c",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
